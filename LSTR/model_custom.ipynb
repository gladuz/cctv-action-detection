{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_uOUT2NpFzZs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CBV3-KCGFzXh"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "        # Self Attention Mechanism\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        # Feed-forward Network\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        # Normalization and Dropout Layers\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # Self Attention Layer with Add & Norm\n",
        "        attn_output, _ = self.self_attn(src, src, src)\n",
        "        src = src + self.dropout1(attn_output)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        # Feed-forward Layer with Add & Norm\n",
        "        ff_output = self.linear2(self.dropout(torch.relu(self.linear1(src))))\n",
        "        src = src + self.dropout2(ff_output)\n",
        "        src = self.norm2(src)\n",
        "\n",
        "        return src\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gkMe41h2GW90"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_layers, dim_feedforward=2048, dropout=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        # EncoderLayer를 num_layers만큼 반복\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)  # 최종 출력 전에 추가되는 normalization layer\n",
        "\n",
        "    def forward(self, src):\n",
        "        for layer in self.layers:\n",
        "            src = layer(src)\n",
        "        return self.norm(src)  # 최종적으로 normalization 수행 후 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wppkQkyRF3q2"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "\n",
        "        # Self Attention Mechanism\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        # Multi-head Attention Mechanism (for encoder-decoder attention)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        # Feed-forward Network\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        # Normalization and Dropout Layers\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, memory):\n",
        "        # Self Attention Layer with Add & Norm\n",
        "        attn_output, _ = self.self_attn(tgt, tgt, tgt)\n",
        "        tgt = tgt + self.dropout1(attn_output)\n",
        "        tgt = self.norm1(tgt)\n",
        "\n",
        "        # Multi-head Attention Layer with encoder-decoder attention, Add & Norm\n",
        "        attn_output, _ = self.multihead_attn(tgt, memory, memory)\n",
        "        tgt = tgt + self.dropout2(attn_output)\n",
        "        tgt = self.norm2(tgt)\n",
        "\n",
        "        # Feed-forward Layer with Add & Norm\n",
        "        ff_output = self.linear2(self.dropout(torch.relu(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(ff_output)\n",
        "        tgt = self.norm3(tgt)\n",
        "\n",
        "        return tgt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bo9Be_6eGqit"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_layers, dim_feedforward=2048, dropout=0.1):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "\n",
        "        # DecoderLayer를 num_layers만큼 반복\n",
        "        self.layers = nn.ModuleList([TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)  # 최종 출력 전에 추가되는 normalization layer\n",
        "\n",
        "    def forward(self, tgt, memory):\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, memory)  # 각 레이어를 통과하면서 memory(즉, encoder의 출력)를 사용\n",
        "        return self.norm(tgt)  # 최종적으로 normalization 수행 후 반환\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZVLZ5EXkFRAq"
      },
      "outputs": [],
      "source": [
        "# Long-Term and Short-Term Memory\n",
        "class Memory:\n",
        "    def __init__(self, long_term_size, short_term_size):\n",
        "        self.long_term_memory = deque(maxlen=long_term_size)\n",
        "        self.short_term_memory = deque(maxlen=short_term_size)\n",
        "\n",
        "    def update(self, new_feature):\n",
        "        self.short_term_memory.append(new_feature)\n",
        "        if len(self.short_term_memory) == self.short_term_memory.maxlen:\n",
        "            self.long_term_memory.append(self.short_term_memory[0])\n",
        "\n",
        "# LSTR Encoder-Decoder using Transformer\n",
        "class LSTREncoderDecoder(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_layers, num_classes=4):\n",
        "        super(LSTREncoderDecoder, self).__init__()\n",
        "        self.encoder_layer = TransformerEncoderLayer(d_model, nhead)\n",
        "        self.encoder = TransformerEncoder(d_model=2560, nhead=8, num_layers=num_layers)#(self.encoder_layer, num_layers=num_layers)\n",
        "        self.decoder_layer = TransformerDecoderLayer(d_model, nhead)\n",
        "        self.decoder = TransformerDecoder(d_model=2560, nhead=8, num_layers=num_layers)#(self.decoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # classifier (4 classes)\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, long_term_memory, short_term_memory):\n",
        "        long_term_encoded = self.encoder(long_term_memory)\n",
        "        decoder_output = self.decoder(short_term_memory, long_term_encoded)\n",
        "\n",
        "        # Assuming the last output is what you want to classify\n",
        "        last_output = decoder_output[-1]\n",
        "        class_output = self.classifier(last_output)\n",
        "        return class_output     # this will be of shape (batch_size, num_classes)\n",
        "\n",
        "# init\n",
        "long_term_size = 2048  # Example size\n",
        "short_term_size = 32  # Example size\n",
        "memory = Memory(long_term_size, short_term_size)\n",
        "\n",
        "d_model = 2048 + 512  # Embedding dimension\n",
        "nhead = 8  # Number of heads in multi-head attention\n",
        "num_layers = 3  # Number of transformer layers\n",
        "\n",
        "model = LSTREncoderDecoder(d_model, nhead, num_layers)\n",
        "\n",
        "# Simulate feature update\n",
        "for _ in range(short_term_size):  # iterate over short-term memory size at least it is filled\n",
        "    rgb_feature = torch.rand(1, 2048)  # RGB features\n",
        "    optical_flow_feature = torch.rand(1, 512)  # Optical Flow features\n",
        "\n",
        "    # Combine or process features if needed\n",
        "    combined_feature = torch.cat((rgb_feature, optical_flow_feature), dim=1)  # Example\n",
        "\n",
        "    memory.update(combined_feature)\n",
        "\n",
        "# Forward pass\n",
        "if len(memory.long_term_memory) > 0 and len(memory.short_term_memory) > 0:\n",
        "    long_term_memory_tensor = torch.stack(list(memory.long_term_memory))\n",
        "    short_term_memory_tensor = torch.stack(list(memory.short_term_memory))\n",
        "    class_output = model(long_term_memory_tensor, short_term_memory_tensor)\n",
        "    class_probabilities = nn.Softmax(dim=1)(class_output)\n",
        "else:\n",
        "    print(\"Memory is not yet filled. Continue collecting features.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUaB07TeFRAr",
        "outputId": "c909dd8f-965b-4c87-ef3e-91a7e4927561"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0973, -0.9920,  0.0909,  0.3911]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsBUzYBZFRAs",
        "outputId": "e1ba98d6-137c-4822-84a5-66e6797df6fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.2724, 0.0916, 0.2706, 0.3654]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv_yVtypHVVP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "abb",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
